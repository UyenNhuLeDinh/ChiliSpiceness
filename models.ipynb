{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a25b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df4585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport the data\n",
    "df_cleaned = pd.read_csv('train.csv')\n",
    "\n",
    "num_features = df_cleaned.select_dtypes(include='number').drop('Scoville Heat Units (SHU)', axis = 1).columns\n",
    "\n",
    "for col in num_features:\n",
    "    # Impute missing values with median\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "    \n",
    "    # Remove outliers\n",
    "    q99 = df_cleaned[col].quantile(0.995)\n",
    "    q01 = df_cleaned[col].quantile(0.001)\n",
    "\n",
    "    # Filter rows — keep only values within the quantile range\n",
    "    df_cleaned = df_cleaned[(df_cleaned[col] <= q99) & (df_cleaned[col] >= q01)]\n",
    "    \n",
    "DROP_COLS = ['color', 'Harvest Time', 'Average Temperature During Storage (celcius)']\n",
    "drop_columns = FunctionTransformer(lambda df: df.drop(columns=DROP_COLS, axis = 1))\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('drop_cols', drop_columns),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c0e5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X = df_cleaned.drop(columns=['Scoville Heat Units (SHU)'])\n",
    "y = df_cleaned['Scoville Heat Units (SHU)']\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9debfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate_model(name, model, param_grid):\n",
    "    print(f\"\\n Grid Search for {name}\")\n",
    "    \n",
    "    pipe = Pipeline ([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Adjust param grid to reflect the pipeline's step name\n",
    "    param_grid = {f'regressor__{key}': val for key, val in param_grid.items()}\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, cv=5,\n",
    "                        scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ecae6",
   "metadata": {},
   "source": [
    "### (A) REGRESSION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4caefa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Grid Search for Linear Regression\n",
      "Best Params: {}\n",
      "MSE: 9506577332.59\n",
      "R² Score: 0.2187\n",
      "\n",
      " Grid Search for Ridge Regression\n",
      "Best Params: {'regressor__alpha': 10}\n",
      "MSE: 9504270625.54\n",
      "R² Score: 0.2189\n",
      "\n",
      " Grid Search for Lasso Regression\n",
      "Best Params: {'regressor__alpha': 10}\n",
      "MSE: 9506554709.58\n",
      "R² Score: 0.2187\n",
      "\n",
      " Grid Search for Decision Tree\n",
      "Best Params: {'regressor__max_depth': 3}\n",
      "MSE: 10352561694.54\n",
      "R² Score: 0.1492\n",
      "\n",
      " Grid Search for Random Forest\n",
      "Best Params: {'regressor__max_depth': 10, 'regressor__n_estimators': 100}\n",
      "MSE: 9308571875.01\n",
      "R² Score: 0.2350\n",
      "\n",
      " Grid Search for Gradient Boosting\n",
      "Best Params: {'regressor__learning_rate': 0.05, 'regressor__max_depth': 3, 'regressor__n_estimators': 50}\n",
      "MSE: 9497100049.68\n",
      "R² Score: 0.2195\n",
      "\n",
      " Grid Search for Support Vector Regressor\n",
      "Best Params: {'regressor__C': 10, 'regressor__kernel': 'linear'}\n",
      "MSE: 16740657601.36\n",
      "R² Score: -0.3758\n"
     ]
    }
   ],
   "source": [
    "# 1. Linear Regression\n",
    "lr_model = evaluate_model(\"Linear Regression\", LinearRegression(), {})\n",
    "\n",
    "# 2. Ridge Regression\n",
    "ridge_model = evaluate_model(\"Ridge Regression\", Ridge(), {'alpha': [0.01, 0.1, 1, 10]})\n",
    "\n",
    "# 3. Lasso Regression\n",
    "lasso_model = evaluate_model(\"Lasso Regression\", Lasso(), {'alpha': [0.01, 0.1, 1, 10]})\n",
    "\n",
    "# 4. Decision Tree\n",
    "dt_model = evaluate_model(\"Decision Tree\", DecisionTreeRegressor(random_state=42), {\n",
    "    'max_depth': [3, 5, 10, None]\n",
    "})\n",
    "\n",
    "# 5. Random Forest\n",
    "rf_model = evaluate_model(\"Random Forest\", RandomForestRegressor(random_state=42), {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10]\n",
    "})\n",
    "\n",
    "# 6. Gradient Boosting\n",
    "gb_model = evaluate_model(\"Gradient Boosting\", GradientBoostingRegressor(random_state=42), {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5]\n",
    "})\n",
    "\n",
    "# 7. Support Vector Regressor\n",
    "svr_model = evaluate_model(\"Support Vector Regressor\", SVR(), {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2292938e",
   "metadata": {},
   "source": [
    "### (B) Multi-class classification analysis with an ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78874246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 100}\n",
      "Accuracy score: 0.49732620320855614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74        92\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         6\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       0.00      0.00      0.00         5\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         4\n",
      "          13       1.00      0.25      0.40         4\n",
      "          14       0.00      0.00      0.00         8\n",
      "          15       0.00      0.00      0.00         7\n",
      "          16       0.00      0.00      0.00         7\n",
      "          17       0.00      0.00      0.00         3\n",
      "          18       0.00      0.00      0.00         5\n",
      "          19       0.00      0.00      0.00         3\n",
      "          20       0.00      0.00      0.00         5\n",
      "          21       0.00      0.00      0.00         3\n",
      "          23       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.50       187\n",
      "   macro avg       0.07      0.05      0.05       187\n",
      "weighted avg       0.31      0.50      0.38       187\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define bin count\n",
    "num_bin = 50\n",
    "X_binned = X.copy()\n",
    "y_binned = pd.qcut(y, q=num_bin, labels=False, duplicates='drop')\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binned, y_binned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build classification pipeline\n",
    "clf_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [5, 10]\n",
    "}\n",
    "\n",
    "grid_clf = GridSearchCV(clf_pipeline, param_grid=param_grid_rf, \n",
    "                        cv=5, scoring='f1_macro')\n",
    "\n",
    "grid_clf.fit(X_train, y_train)\n",
    "print('Best hyperparameters:', grid_clf.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = grid_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score:', accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26fc192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 bins → Accuracy: 0.7390\n",
      "5 bins → Accuracy: 0.6545\n",
      "7 bins → Accuracy: 0.6385\n",
      "10 bins → Accuracy: 0.5947\n"
     ]
    }
   ],
   "source": [
    "for bins in [3, 5, 7, 10]:\n",
    "    y_binned = pd.qcut(y, q=bins, labels=False, duplicates= 'drop')\n",
    "    scores = cross_val_score(clf_pipeline, X, y_binned, cv=5, scoring='accuracy')\n",
    "    print(f\"{bins} bins → Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c530d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "# Bin original y (for training and mapping reference)\n",
    "y_binned = pd.qcut(y, q=num_bin, labels=False, duplicates='drop')\n",
    "\n",
    "# Now, get the bin intervals from the dtype of the binned series\n",
    "bin_intervals = pd.qcut(y, q=num_bin, duplicates='drop').dtype.categories\n",
    "\n",
    "bin_midpoints = [interval.mid for interval in bin_intervals]\n",
    "\n",
    "# Predict bin labels on unseen/test data\n",
    "y_pred_bins = grid_clf.predict(df_test) \n",
    "\n",
    "# Convert bin label to SHU approximation using bin midpoints\n",
    "y_pred_shu = [bin_midpoints[int(bin_label)] for bin_label in y_pred_bins]\n",
    "\n",
    "y_test_kaggle = pd.DataFrame(y_pred_shu, columns=[\"Scoville Heat Units (SHU)\"])\n",
    "y_test_kaggle.index.name = \"index\"\n",
    "y_test_kaggle[['Scoville Heat Units (SHU)']].to_csv(\"kaggle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cae54d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntervalIndex([      (-0.001, 1687.746],    (1687.746, 11696.862],\n",
       "                 (11696.862, 21014.122],    (21014.122, 33524.02],\n",
       "                  (33524.02, 44015.419],   (44015.419, 55603.204],\n",
       "                 (55603.204, 69801.462],   (69801.462, 81131.709],\n",
       "                 (81131.709, 92165.908],  (92165.908, 104195.538],\n",
       "               (104195.538, 115786.399],  (115786.399, 130505.42],\n",
       "                (130505.42, 144631.227], (144631.227, 156024.772],\n",
       "               (156024.772, 172074.327], (172074.327, 181797.978],\n",
       "               (181797.978, 195615.438],  (195615.438, 223328.71],\n",
       "                (223328.71, 242854.924], (242854.924, 264765.219],\n",
       "               (264765.219, 290983.956], (290983.956, 325945.179],\n",
       "               (325945.179, 377966.343],  (377966.343, 527639.86]],\n",
       "              dtype='interval[float64, right]')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab1ea7d",
   "metadata": {},
   "source": [
    "**Comment**: \n",
    "\n",
    "Accuracy vs. Number of Bins\n",
    "\n",
    "As the number of bins decreases (e.g., from 10 to 3), classification accuracy tends to increase. This is because it's easier for the classifier to correctly predict broader categories. For example, since a large portion of the dataset consists of peppers with a SHU of 0, the classifier can perform well simply by assigning many samples to the first bin (which includes 0). This boosts accuracy but oversimplifies the predictions.\n",
    "\n",
    "\n",
    "Impact on SHU Prediction Error (MAE or MSE)\n",
    "\n",
    "However, when we convert bin predictions back into SHU values (e.g., using the midpoint of each interval), this coarse binning becomes problematic. If the first bin covers a wide range, say SHU 0 to 1600, and its midpoint is around 800, then all predictions in that bin are assigned ~800 — even though many actual values are 0. This creates large errors in SHU estimation (e.g., MAE or MSE).\n",
    "\n",
    "\n",
    "Effect of Increasing Number of Bins\n",
    "\n",
    "Using more bins creates finer granularity, allowing the model to make more precise distinctions between SHU ranges. This reduces the error when mapping bins back to SHU values. However, as the number of bins increases too much (e.g., beyond 100), bins may become too narrow, class imbalance increases, and the classifier may struggle. As a result, SHU prediction error (e.g., on Kaggle) stops improving significantly beyond a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a840a0f",
   "metadata": {},
   "source": [
    "### (C) A two-step analysis (two sequential pipelines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95a18fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train_mapped = np.where(y_train > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1773ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid1 = {\n",
    "'classifier__n_estimators': [100, 200],\n",
    "'classifier__max_depth': [10, 20],\n",
    "'classifier__min_samples_split': [5, 10],\n",
    "'classifier__min_samples_leaf': [2, 4],\n",
    "}\n",
    "\n",
    "gs_class = GridSearchCV(class_pipe, param_grid1, cv=5, scoring='accuracy')\n",
    "gs_class.fit(X_train, y_train_mapped)\n",
    "\n",
    "\n",
    "best_classifier = gs_class.best_estimator_\n",
    "class_pred = best_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84070832",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [438, 935]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m X_reg \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[spicy_peppers_indices]\n\u001b[1;32m      4\u001b[0m y_reg \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[spicy_peppers_indices]\n\u001b[0;32m----> 6\u001b[0m X_reg_train, X_reg_test, y_reg_train, y_reg_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_split.py:2872\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2868\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2870\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2872\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2874\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2877\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2878\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2879\u001b[0m     )\n\u001b[1;32m   2880\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/model_selection/_split.py:1908\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m \n\u001b[1;32m   1881\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m   1907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1908\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1909\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:532\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    531\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    478\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [438, 935]"
     ]
    }
   ],
   "source": [
    "spicy_peppers_indices = np.where(class_pred == 1)[0]\n",
    "\n",
    "X_reg = X.iloc[spicy_peppers_indices]\n",
    "y_reg = y.iloc[spicy_peppers_indices]\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980a75b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_reg_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m param_grid2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__n_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m],\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__min_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor__min_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m gs_reg \u001b[38;5;241m=\u001b[39m GridSearchCV(reg_pipe, param_grid2, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m gs_reg\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_reg_train\u001b[49m, y_reg_train)\n\u001b[1;32m     16\u001b[0m best_regressor \u001b[38;5;241m=\u001b[39m gs_reg\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     17\u001b[0m reg_pred \u001b[38;5;241m=\u001b[39m best_regressor\u001b[38;5;241m.\u001b[39mpredict(X_reg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_reg_train' is not defined"
     ]
    }
   ],
   "source": [
    "reg_pipe = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid2 = {\n",
    "'regressor__n_estimators': [100, 200],\n",
    "'regressor__max_depth': [10, 20],\n",
    "'regressor__min_samples_split': [5, 10],\n",
    "'regressor__min_samples_leaf': [2, 4],\n",
    "}\n",
    "\n",
    "gs_reg = GridSearchCV(reg_pipe, param_grid2, cv=5, scoring= 'neg_mean_squared_error')\n",
    "gs_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "best_regressor = gs_reg.best_estimator_\n",
    "reg_pred = best_regressor.predict(X_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e469d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
